{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Masked Language Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFxy9VfywWj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f98c95-3a15-4ee1-9b12-5b5af11a9045"
      },
      "source": [
        "folder = 'multi_cased_L-12_H-768_A-12'\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n",
        "\n",
        "print('Downloading model...')\n",
        "zip_path = '{}.zip'.format(folder)\n",
        "!test -d $folder || (wget $download_url && unzip $zip_path)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
        "\n",
        "!pip install keras-bert\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "import tokenization\n",
        "\n",
        "config_path = folder+'/bert_config.json'\n",
        "checkpoint_path = folder+'/bert_model.ckpt'\n",
        "vocab_path = folder+'/vocab.txt'\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.gfile = tf.io.gfile\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
        "\n",
        "# загружаем модель\n",
        "print('Loading model...')\n",
        "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
        "\n",
        "print('OK')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "--2021-05-22 17:19:12--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.128, 142.250.141.128, 2607:f8b0:4023:c06::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M  82.4MB/s    in 7.2s    \n",
            "\n",
            "2021-05-22 17:19:20 (88.2 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n",
            "--2021-05-22 17:19:27--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py’\n",
            "\n",
            "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-05-22 17:19:27 (18.8 MB/s) - ‘tokenization.py’ saved [12257/12257]\n",
            "\n",
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.19.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp37-none-any.whl size=34144 sha256=0a013ad7e9a688768b25337771b5206562f57d3f2013ee16b6b03f9521ea113d\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp37-none-any.whl size=12942 sha256=3d9f7ff1ef0e751a649ca4cd66e02f3080511bb5f2bfd882cec81cf4c6019685\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp37-none-any.whl size=7554 sha256=138c659595a3f05095416be0d9b74b09e6b2663d4d2c00e18bb674a62822fe2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp37-none-any.whl size=15611 sha256=0853adfb1cfd81e78a86560ff5a80ac0d2bf0b052270773ac1fd0908d60e7e55\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp37-none-any.whl size=5269 sha256=a175fc4f379919dd52c52ce0e4b9464149e1505f2c3786e05d5ec449cdcccd74\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp37-none-any.whl size=5623 sha256=005c5c5a426ec82adb9bdd99b52c7991ec6411f9e192f15c1bf83bea841daa99\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp37-none-any.whl size=4558 sha256=5e8eb1fceee34b3e77a19b23051ce28c1e03289b10cd2ba79e2e9d2a412c2507\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp37-none-any.whl size=17278 sha256=973058b2043c58b81536641b1f28d6359a6b46f18441ea97dc43f8d6b2f9cc9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
            "Loading model...\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBCsGmQEFIso",
        "outputId": "6cfb60d2-036a-4d0f-a9f5-bf5527cd94b9"
      },
      "source": [
        "# входная фраза с закрытыми словами с помощью [MASK]\n",
        "sentence = 'Я-то думал он принимает мою позицию, а он даже [MASK] не принимает'\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "\n",
        "sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')\n",
        "sentence = sentence.split('[MASK]')\n",
        "tokens = ['[CLS]']\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "    if i == 0:\n",
        "        tokens = tokens + tokenizer.tokenize(sentence[i]) \n",
        "    else:\n",
        "        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) \n",
        "tokens = tokens + ['[SEP]']\n",
        "\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)        \n",
        "\n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "mask_input = [0]*512\n",
        "for i in range(len(mask_input)):\n",
        "    if token_input[i] == 103:\n",
        "        mask_input[i] = 1\n",
        "\n",
        "seg_input = [0]*512\n",
        "\n",
        "\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "\n",
        "\n",
        "predicts = predicts[0][:len(tokens)]\n",
        "out = []\n",
        "\n",
        "for i in range(len(mask_input[0])):\n",
        "    if mask_input[0][i] == 1:\n",
        "        out.append(predicts[i]) \n",
        "\n",
        "out = tokenizer.convert_ids_to_tokens(out)\n",
        "out = ' '.join(out)               \n",
        "out = tokenization.printable_text(out)\n",
        "out = out.replace(' ##','')\n",
        "print('Result:', out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Я-то думал он принимает мою позицию, а он даже [MASK] не принимает\n",
            "Result: себя\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}